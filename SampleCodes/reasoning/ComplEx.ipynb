{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ComplEx 实践\n",
    "\n",
    "在这个演示中，我们使用DistMUlt([论文链接](http://proceedings.mlr.press/v48/trouillon16.pdf))对示例中文知识图谱进行链接预测，从而达到补全知识图谱的目的。\n",
    "\n",
    "希望在这个demo中帮助大家了解知识图谱表示学习的作用原理和机制。\n",
    "\n",
    "本demo建议使用python3运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "这个示例中，我们使用的是表示学习模型做知识图谱链接预测常用的benchmark数据集 FB15k-237：\n",
    "\n",
    "| #Ent | #Rel | # Train | #Test | #Valid |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 14,541| 237| 272,115| 17,535| 20,466 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComplEx 原理回顾\n",
    "DistMult将实体和关系映射到实数空间，并假设 $\\mathbf{h}\\mathbf{M}_r = \\mathbf{t}$, 计算一个三元组$(h,r,t)$的得分函数为：$f_r(h,t) = \\mathbf{h}\\mathbf{M}_r \\mathbf{t} $, 由于当将$\\mathbf{M}_r$设置为对角矩阵时，$\\mathbf{h}\\mathbf{M}_r \\mathbf{t} = \\mathbf{t}\\mathbf{M}_r \\mathbf{h}$的，隐含着每个关系都是对称关系的结论，这显然是不合适的。\n",
    "\n",
    "ComplEx通过将实体和关系映射到复数空间解决了这个问题，因为复数空间的向量和矩阵的计算是不满足交换律的。ComplEx的得分函数设置如下：\n",
    "$$ \\phi(h,r,t) = Re(<h,r,\\bar{t} >)  $$\n",
    "$$ \\qquad \\qquad \\qquad = <Re(h), Re(r), Re(t)> $$ \n",
    "$$ \\qquad \\qquad \\qquad \\qquad + <Im(h), Re(r), Im(t)> $$\n",
    "$$ \\qquad \\qquad \\qquad \\qquad + <Re(h), Im(r), Im(t)> $$\n",
    "$$ \\qquad \\qquad \\qquad \\qquad - <Im(h), Im(r), Re(t)> $$\n",
    "其中$Re(x)$表示$x$的实数部分，$Im(x)$表示$x$的虚数部分，$\\bar{x}$表示$x$的共轭复数。\n",
    "\n",
    "ComplEx的损失函数如下：\n",
    "$$ L = \\frac{1}{len(S)}\\sum_{(h,r,t)\\in \\{S, S^\\prime\\}}log(e^{(- f_r(h,t)*lable)} + 1)$$\n",
    "其中正样本的lable为1，负样本的lable为-1.优化目标为最小化损失函数$L$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码实践"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangwen/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import time \n",
    "import argparse\n",
    "import random\n",
    "import numpy as np \n",
    "import os.path\n",
    "import math\n",
    "import timeit\n",
    "from multiprocessing import JoinableQueue, Queue, Process\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistMult:\n",
    "    @property\n",
    "    def variables(self):\n",
    "        return self.__variables\n",
    "\n",
    "    @property\n",
    "    def num_triple_train(self):\n",
    "        return self.__num_triple_train\n",
    "\n",
    "    @property \n",
    "    def num_triple_test(self):\n",
    "        return self.__num_triple_test\n",
    "\n",
    "    @property\n",
    "    def testing_data(self):\n",
    "        return self.__triple_test\n",
    "\n",
    "    @property \n",
    "    def num_entity(self):\n",
    "        return self.__num_entity\n",
    "\n",
    "    @property\n",
    "    def embedding_entity(self):\n",
    "        return self.__embedding_entity\n",
    "\n",
    "\n",
    "    @property\n",
    "    def embedding_relation(self):\n",
    "        return self.__embedding_relation\n",
    "\n",
    "    @property\n",
    "    def hr_t(self):\n",
    "        return self.__hr_t\n",
    "\n",
    "    @property \n",
    "    def tr_h(self):\n",
    "        return self.__tr_h\n",
    "    \n",
    "    @property\n",
    "    def entity2id(self):\n",
    "        return self.__entity2id\n",
    "    \n",
    "    @property\n",
    "    def relation2id(self):\n",
    "        return self.__relation2id\n",
    "\n",
    "    @property\n",
    "    def id2entity(self):\n",
    "        return self.__id2entity\n",
    "    \n",
    "    @property\n",
    "    def id2relation(self):\n",
    "        return self.__id2relation\n",
    "\n",
    "    def training_data_batch(self, batch_size = 512):\n",
    "        n_triple = len(self.__triple_train)\n",
    "        rand_idx = np.random.permutation(n_triple)\n",
    "        start = 0\n",
    "        while start < n_triple:\n",
    "            start_t = timeit.default_timer()\n",
    "            end = min(start+batch_size, n_triple)\n",
    "            size = end - start \n",
    "            train_triple_positive = np.asarray([ self.__triple_train[x] for x in  rand_idx[start:end]])\n",
    "\n",
    "            num_neg = 1\n",
    "            train_negative1 = np.repeat(train_triple_positive, num_neg, axis=0)\n",
    "            train_negative2 = np.repeat(train_triple_positive, num_neg, axis=0)\n",
    "            train_negative1[:, 0] = np.random.randint(self.__num_entity, size=num_neg*size)\n",
    "            train_negative2[:, 2] = np.random.randint(self.__num_entity, size=num_neg*size)\n",
    "            train_triple_negative = np.concatenate((train_negative1, train_negative2), axis=0)\n",
    "\n",
    "            start = end\n",
    "            prepare_t = timeit.default_timer()-start_t\n",
    "\n",
    "            yield train_triple_positive, train_triple_negative, prepare_t\n",
    "\n",
    "\n",
    "    def __init__(self, data_dir, negative_sampling,learning_rate, \n",
    "             batch_size, max_iter, margin, dimension, norm, evaluation_size, regularizer_weight):\n",
    "        # this part for data prepare\n",
    "        self.__data_dir=data_dir\n",
    "        self.__negative_sampling=negative_sampling\n",
    "        self.__regularizer_weight = regularizer_weight\n",
    "        self.__norm = norm\n",
    "\n",
    "        self.__entity2id={}\n",
    "        self.__id2entity={}\n",
    "        self.__relation2id={}\n",
    "        self.__id2relation={}\n",
    "\n",
    "        self.__triple_train=[] #[(head_id, relation_id, tail_id),...]\n",
    "        self.__triple_test=[]\n",
    "        self.__triple_valid=[]\n",
    "        self.__triple = []\n",
    "\n",
    "        self.__num_entity=0\n",
    "        self.__num_relation=0\n",
    "        self.__num_triple_train=0\n",
    "        self.__num_triple_test=0\n",
    "        self.__num_triple_valid=0\n",
    "\n",
    "        # load all the file: entity2id.txt, relation2id.txt, train.txt, test.txt, valid.txt\n",
    "        self.load_data()\n",
    "        print('finish preparing data. ')\n",
    "\n",
    "\n",
    "        # this part for the model:\n",
    "        self.__learning_rate = learning_rate\n",
    "        self.__batch_size = batch_size\n",
    "        self.__max_iter = max_iter\n",
    "        self.__margin = margin\n",
    "        self.__dimension = dimension\n",
    "        self.__variables= []\n",
    "        #self.__norm = norm\n",
    "        self.__evaluation_size = evaluation_size\n",
    "        bound = 6 / math.sqrt(self.__dimension)\n",
    "        with tf.device('/cpu'):\n",
    "            self.__embedding_entity = tf.get_variable('embedding_entity', [self.__num_entity, self.__dimension*2],\n",
    "                                                       initializer=tf.random_uniform_initializer(minval=-bound, maxval=bound, seed = 123), dtype=tf.float32)\n",
    "            self.__embedding_relation = tf.get_variable('embedding_relation', [self.__num_relation, self.__dimension*2],\n",
    "                                                         initializer=tf.random_uniform_initializer(minval=-bound, maxval=bound, seed =124), dtype=tf.float32)\n",
    "            self.__variables.append(self.__embedding_entity)\n",
    "            self.__variables.append(self.__embedding_relation)\n",
    "            print('finishing initializing')\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        print('loading entity2id.txt ...')\n",
    "        with open(os.path.join(self.__data_dir, 'entity2id.txt'), encoding='utf-8') as f:\n",
    "            self.__entity2id = {line.strip().split('\\t')[0]: int(line.strip().split('\\t')[1]) for line in f.readlines()}\n",
    "            self.__id2entity = {value:key for key,value in self.__entity2id.items()}\n",
    "\n",
    "        print('loading reltion2id.txt ...')     \n",
    "        with open(os.path.join(self.__data_dir,'relation2id.txt'), encoding='utf-8') as f:\n",
    "            self.__relation2id = {line.strip().split('\\t')[0]: int(line.strip().split('\\t')[1]) for line in f.readlines()}\n",
    "            self.__id2relation = {value:key for key, value in self.__relation2id.items()}\n",
    "\n",
    "        def load_triple(self, triplefile):\n",
    "            triple_list = [] #[(head_id, relation_id, tail_id),...]\n",
    "            with open(os.path.join(self.__data_dir, triplefile), encoding='utf-8') as f:\n",
    "                for line in f.readlines():\n",
    "                    line_list = line.strip().split('\\t')\n",
    "                    assert len(line_list) == 3\n",
    "                    headid = self.__entity2id[line_list[0]]\n",
    "                    relationid = self.__relation2id[line_list[1]]\n",
    "                    tailid = self.__entity2id[line_list[2]]\n",
    "                    triple_list.append((headid, relationid, tailid))\n",
    "                    self.__hr_t[(headid, relationid)].add(tailid)\n",
    "                    self.__tr_h[(tailid, relationid)].add(headid)\n",
    "            return triple_list\n",
    "\n",
    "        self.__hr_t = defaultdict(set)\n",
    "        self.__tr_h = defaultdict(set)\n",
    "        self.__triple_train = load_triple(self, 'train.txt')\n",
    "        self.__triple_test = load_triple(self, 'test.txt')\n",
    "        self.__triple_valid = load_triple(self, 'valid.txt')\n",
    "        self.__triple = np.concatenate([self.__triple_train, self.__triple_test, self.__triple_valid], axis = 0 )\n",
    "\n",
    "        self.__num_relation = len(self.__relation2id)\n",
    "        self.__num_entity = len(self.__entity2id)\n",
    "        self.__num_triple_train = len(self.__triple_train)\n",
    "        self.__num_triple_test = len(self.__triple_test)\n",
    "        self.__num_triple_valid = len(self.__triple_valid)\n",
    "\n",
    "        print('entity number: ' + str(self.__num_entity))\n",
    "        print('relation number: ' + str(self.__num_relation))\n",
    "        print('training triple number: ' + str(self.__num_triple_train))\n",
    "        print('testing triple number: ' + str(self.__num_triple_test))\n",
    "        print('valid triple number: ' + str(self.__num_triple_valid))\n",
    "\n",
    "\n",
    "        if self.__negative_sampling == 'bern':\n",
    "            self.__relation_property_head = {x:[] for x in range(self.__num_relation)} #{relation_id:[headid1, headid2,...]}\n",
    "            self.__relation_property_tail = {x:[] for x in range(self.__num_relation)} #{relation_id:[tailid1, tailid2,...]}\n",
    "            self.__relation_property = {x:[] for x in range(self.__num_relation)} \n",
    "            for t in self.__triple_train:\n",
    "                #print(t)\n",
    "                self.__relation_property_head[t[1]].append(t[0])\n",
    "                self.__relation_property_tail[t[1]].append(t[2])\n",
    "            #print(self.__relation_property_head[0])\n",
    "            #print(self.__relation_property_tail[0])\n",
    "            for x in self.__relation_property_head.keys():\n",
    "                t = len(set(self.__relation_property_tail[x]))\n",
    "                h = len(set(self.__relation_property_head[x]))\n",
    "                self.__relation_property[x] = float(t)/(h+t+0.000000001)\n",
    "            #self.__relation_property = {x:(len(set(self.__relation_property_tail[x])))/(len(set(self.__relation_property_head[x]))+ len(set(self.__relation_property_tail[x]))) \\\n",
    "            #\t\t\t\t\t\t\t for x in self.__relation_property_head.keys()} # {relation_id: p, ...} 0< num <1, and for relation replace head entity with the property p\n",
    "        else: \n",
    "            print(\"unif set don't need to calculate hpt and tph\")\n",
    "\n",
    "    def Re(self,x):\n",
    "        return x[:, :self.__dimension]\n",
    "    def Im(self,x):\n",
    "        return x[:, self.__dimension :]\n",
    "    def score(self,h,r,t):\n",
    "        h = tf.reshape(h, [-1, self.__dimension*2])\n",
    "        r = tf.reshape(r, [-1, self.__dimension*2])\n",
    "        t = tf.reshape(t, [-1, self.__dimension*2])\n",
    "        return tf.reduce_sum(self.Re(h)*self.Re(r)*self.Re(t)\n",
    "                                + self.Im(h)*self.Re(r)*self.Im(t)\n",
    "                                + self.Re(h)*self.Im(r)*self.Im(t)\n",
    "                                - self.Im(h)*self.Im(r)*self.Re(t),\n",
    "                            axis = -1) \n",
    "\n",
    "    def train(self, inputs):\n",
    "        embedding_relation = self.__embedding_relation\n",
    "        embedding_entity = self.__embedding_entity\n",
    "\n",
    "        triple_positive, triple_negative = inputs # triple_positive:(head_id,relation_id,tail_id)\n",
    "\n",
    "        #norm_entity = tf.nn.l2_normalize(embedding_entity, dim = 1)\n",
    "        #norm_relation = tf.nn.l2_normalize(embedding_relation, dim = 1)\n",
    "        norm_entity = embedding_entity\n",
    "        norm_relation = embedding_relation\n",
    "        norm_entity_l2sum = tf.sqrt(tf.reduce_sum(norm_entity**2, axis = 1))\n",
    "\n",
    "        embedding_positive_head = tf.nn.embedding_lookup(norm_entity, triple_positive[:, 0])\n",
    "        embedding_positive_tail = tf.nn.embedding_lookup(norm_entity, triple_positive[:, 2])\n",
    "        embedding_positive_relation = tf.nn.embedding_lookup(norm_relation, triple_positive[:, 1])\n",
    "\n",
    "        embedding_negative_head = tf.nn.embedding_lookup(norm_entity, triple_negative[:, 0])\n",
    "        embedding_negative_tail = tf.nn.embedding_lookup(norm_entity, triple_negative[:, 2])\n",
    "        embedding_negative_relation = tf.nn.embedding_lookup(norm_relation, triple_negative[:, 1])\n",
    "\n",
    "        # score_positive = tf.reduce_sum(embedding_positive_head * embedding_positive_relation * embedding_positive_tail, axis = 1)\n",
    "        # score_negative = tf.reduce_sum(embedding_negative_head * embedding_negative_relation * embedding_negative_tail, axis = 1)\n",
    "        score_positive = self.score(embedding_positive_head, embedding_positive_relation, embedding_positive_tail)\n",
    "        score_negative = self.score(embedding_negative_head, embedding_negative_relation, embedding_negative_tail)\n",
    "        score = tf.concat((-score_positive, score_negative), axis =0)\n",
    "        \n",
    "        loss_triple = tf.reduce_mean(tf.nn.softplus(score))\n",
    "        \n",
    "        self.__loss_regularizer = loss_regularizer = tf.reduce_sum(tf.abs(self.__embedding_relation)) + tf.reduce_sum(tf.abs(self.__embedding_entity))\n",
    "        return loss_triple + loss_regularizer*self.__regularizer_weight,  norm_entity_l2sum\n",
    "\n",
    "    def test(self, inputs):\n",
    "        embedding_relation = self.__embedding_relation\n",
    "        embedding_entity = self.__embedding_entity\n",
    "\n",
    "        triple_test = inputs # (headid, tailid, tailid)\n",
    "        head_vec = tf.nn.embedding_lookup(embedding_entity, triple_test[0])\n",
    "        rel_vec = tf.nn.embedding_lookup(embedding_relation, triple_test[1])\n",
    "        tail_vec = tf.nn.embedding_lookup(embedding_entity, triple_test[2])\n",
    "\n",
    "        norm_embedding_entity = tf.nn.l2_normalize(embedding_entity, dim =1 )\n",
    "        norm_embedding_relation = tf.nn.l2_normalize(embedding_relation, dim = 1)\n",
    "        norm_head_vec = tf.nn.embedding_lookup(norm_embedding_entity, triple_test[0])\n",
    "        norm_rel_vec = tf.nn.embedding_lookup(norm_embedding_relation, triple_test[1])\n",
    "        norm_tail_vec = tf.nn.embedding_lookup(norm_embedding_entity, triple_test[2])\n",
    "        \n",
    "        score_head = self.score(embedding_entity,  rel_vec, tail_vec)\n",
    "        score_tail = self.score(head_vec, rel_vec,  embedding_entity)\n",
    "        norm_score_head = self.score(norm_embedding_entity,  norm_rel_vec, norm_tail_vec)\n",
    "        norm_score_tail = self.score(norm_head_vec, norm_rel_vec,  norm_embedding_entity)\n",
    "        \n",
    "        \n",
    "        # _, id_replace_head = tf.nn.top_k(tf.reduce_sum(embedding_entity * rel_vec * tail_vec, axis=1), k=self.__num_entity)\n",
    "        # _, id_replace_tail = tf.nn.top_k(tf.reduce_sum(head_vec * rel_vec * embedding_entity, axis=1), k=self.__num_entity)\n",
    "        _, id_replace_head = tf.nn.top_k(score_head, k=self.__num_entity)\n",
    "        _, id_replace_tail = tf.nn.top_k(score_tail, k=self.__num_entity)\n",
    "        \n",
    "        \n",
    "        #_, norm_id_replace_head = tf.nn.top_k(tf.reduce_sum(norm_embedding_entity * norm_rel_vec * norm_tail_vec, axis=1), k=self.__num_entity)\n",
    "        #_, norm_id_replace_tail = tf.nn.top_k(tf.reduce_sum(norm_head_vec * norm_rel_vec * norm_embedding_entity, axis=1), k=self.__num_entity)\n",
    "        _, norm_id_replace_head = tf.nn.top_k(norm_score_head, k=self.__num_entity)\n",
    "        _, norm_id_replace_tail = tf.nn.top_k(norm_score_tail, k=self.__num_entity)\n",
    "            \n",
    "        return id_replace_head, id_replace_tail, norm_id_replace_head, norm_id_replace_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_operation(model, learning_rate=0.01, margin=1.0, optimizer_str = 'gradient'):\n",
    "    with tf.device('/cpu'):\n",
    "        train_triple_positive_input = tf.placeholder(tf.int32, [None, 3])\n",
    "        train_triple_negative_input = tf.placeholder(tf.int32, [None, 3])\n",
    "\n",
    "        loss, norm_entity = model.train([train_triple_positive_input, train_triple_negative_input])\n",
    "        if optimizer_str == 'gradient':\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "        elif optimizer_str == 'rms':\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate = learning_rate)\n",
    "        elif optimizer_str == 'adam':\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Dose not support %s optimizer\" %optimizer_str)\n",
    "\n",
    "        grads = optimizer.compute_gradients(loss, model.variables)\n",
    "        op_train = optimizer.apply_gradients(grads)\n",
    "\n",
    "        return train_triple_positive_input, train_triple_negative_input, loss, op_train, norm_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_operation(model):\n",
    "    with tf.device('/cpu'):\n",
    "        test_triple = tf.placeholder(tf.int32, [3])\n",
    "        head_rank, tail_rank, norm_head_rank, norm_tail_rank = model.test(test_triple)\n",
    "        return test_triple, head_rank, tail_rank, norm_head_rank, norm_tail_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试一个样本函数\n",
    "def test_one_sample(model, trp, session):\n",
    "    t = trp\n",
    "    id_replace_head , id_replace_tail, norm_id_replace_head , norm_id_replace_tail  = session.run([head_rank, tail_rank, norm_head_rank, norm_tail_rank], {test_triple:t})\n",
    "    hr_t = model.hr_t\n",
    "    tr_h = model.tr_h\n",
    "    \n",
    "    hrank = 0\n",
    "    fhrank = 0\n",
    "    predicted_head_tmp = []\n",
    "    for i in range(len(id_replace_head)):\n",
    "        val = id_replace_head[i]\n",
    "        predicted_head_tmp.append(val)\n",
    "        if val == t[0]:\n",
    "            break\n",
    "        else: \n",
    "            hrank += 1\n",
    "            fhrank += 1 \n",
    "            if val in tr_h[(t[2],t[1])]:\n",
    "                fhrank -= 1\n",
    "                _ = predicted_head_tmp.pop()\n",
    "    predicted_head_tmp = [id_replace_head[i] for i in range(len(id_replace_head))]\n",
    "    \n",
    "    norm_hrank = 0\n",
    "    norm_fhrank = 0\n",
    "    norm_predicted_head_tmp = []\n",
    "    for i in range(len(norm_id_replace_head)):\n",
    "        val = norm_id_replace_head[i]\n",
    "        norm_predicted_head_tmp.append(val)\n",
    "        if val == t[0]:\n",
    "            break\n",
    "        else: \n",
    "            norm_hrank += 1\n",
    "            norm_fhrank += 1 \n",
    "            if val in tr_h[(t[2],t[1])]:\n",
    "                norm_fhrank -= 1\n",
    "                _ = norm_predicted_head_tmp.pop()\n",
    "    norm_predicted_head_tmp = [id_replace_head[i] for i in range(len(norm_id_replace_head))]\n",
    "\n",
    "    trank = 0\n",
    "    ftrank = 0\n",
    "    predicted_tail_tmp = []\n",
    "    for i in range(len(id_replace_tail)):\n",
    "        val = id_replace_tail[i]\n",
    "        predicted_tail_tmp.append(val)\n",
    "        if val == t[2]:\n",
    "            break\n",
    "        else:\n",
    "            trank += 1\n",
    "            ftrank += 1\n",
    "            if val in hr_t[(t[0], t[1])]:\n",
    "                ftrank -= 1\n",
    "                _ = predicted_tail_tmp.pop()\n",
    "    predicted_tail_tmp = [id_replace_tail[i] for i in range(len(id_replace_tail))]\n",
    "\n",
    "    norm_trank = 0\n",
    "    norm_ftrank = 0\n",
    "    norm_predicted_tail_tmp = []\n",
    "    for i in range(len(norm_id_replace_tail)):\n",
    "        val = norm_id_replace_tail[i]\n",
    "        norm_predicted_tail_tmp.append(val)\n",
    "        if val == t[2]:\n",
    "            break\n",
    "        else:\n",
    "            norm_trank += 1\n",
    "            norm_ftrank += 1\n",
    "            if val in hr_t[(t[0], t[1])]:\n",
    "                norm_ftrank -= 1\n",
    "                _ = norm_predicted_tail_tmp.pop()\n",
    "    norm_predicted_tail_tmp = [id_replace_tail[i] for i in range(len(norm_id_replace_tail))]\n",
    "    \n",
    "    return hrank+1, fhrank+1, trank+1, ftrank+1, norm_hrank+1, norm_fhrank+1, norm_trank+1, norm_ftrank+1, \\\n",
    "            predicted_head_tmp, predicted_tail_tmp, norm_predicted_head_tmp, norm_predicted_tail_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit(rank_head, rank_tail, k):\n",
    "    n_test = len(rank_head)\n",
    "    assert len(rank_head) == len(rank_tail)\n",
    "    hit_head = np.sum(np.asarray(np.asarray(rank_head)<=k , dtype=np.float32))/n_test\n",
    "    hit_tail = np.sum(np.asarray(np.asarray(rank_tail)<=k , dtype=np.float32))/n_test\n",
    "    hit = (hit_head + hit_tail)/2.0\n",
    "    return hit_head, hit_tail, hit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(n_test):\n",
    "    predicted_tail = []\n",
    "    norm_predicted_tail = []\n",
    "    predicted_head = []\n",
    "    norm_predicted_head = []\n",
    "    np.random.shuffle(testing_data)\n",
    "    for i in range(n_test):\n",
    "        print('[%.2f sec] --- testing[%d/%d]' %(timeit.default_timer()-start, i+1, n_test), end='\\r')\n",
    "        t = testing_data[i]\n",
    "        hrank, fhrank, trank, ftrank, norm_hrank, norm_fhrank, norm_trank, norm_ftrank, \\\n",
    "                predicted_head_tmp, predicted_tail_tmp, norm_predicted_head_tmp, norm_predicted_tail_tmp = test_one_sample(model, t, session)\n",
    "        #print(hrank, fhrank, trank, ftrank, norm_hrank, norm_fhrank, norm_trank, norm_ftrank)\n",
    "        rank_head.append(hrank)\n",
    "        rank_tail.append(trank)\n",
    "        filter_rank_head.append(fhrank)\n",
    "        filter_rank_tail.append(ftrank)\n",
    "        \n",
    "        norm_rank_head.append(norm_hrank)\n",
    "        norm_rank_tail.append(norm_trank)\n",
    "        norm_filter_rank_head.append(norm_fhrank)\n",
    "        norm_filter_rank_tail.append(norm_ftrank)\n",
    "\n",
    "        predicted_tail.append(predicted_tail_tmp)\n",
    "        norm_predicted_tail.append(norm_predicted_tail_tmp)\n",
    "        predicted_head.append(predicted_head_tmp)\n",
    "        norm_predicted_head.append(norm_predicted_head_tmp)\n",
    "    mean_rank_head = np.sum(rank_head, dtype=np.float32)/n_test\n",
    "    mean_rank_tail = np.sum(rank_tail, dtype=np.float32)/n_test\n",
    "    filter_mean_rank_head = np.sum(filter_rank_head, dtype=np.float32)/n_test\n",
    "    filter_mean_rank_tail = np.sum(filter_rank_tail, dtype=np.float32)/n_test\n",
    "\n",
    "    norm_mean_rank_head = np.sum(norm_rank_head, dtype=np.float32)/n_test\n",
    "    norm_mean_rank_tail = np.sum(norm_rank_tail, dtype=np.float32)/n_test\n",
    "    norm_filter_mean_rank_head = np.sum(norm_filter_rank_head, dtype=np.float32)/n_test\n",
    "    norm_filter_mean_rank_tail = np.sum(norm_filter_rank_tail, dtype=np.float32)/n_test\n",
    "\n",
    "    mean_reciprocal_rank_head = np.sum(1.0/np.asarray(rank_head, dtype=np.float32))/n_test\n",
    "    mean_reciprocal_rank_tail = np.sum(1.0/np.asarray(rank_tail, dtype=np.float32))/n_test\n",
    "    filter_mean_reciprocal_rank_head = np.sum(1.0/np.asarray(filter_rank_head, dtype=np.float32))/n_test\n",
    "    filter_mean_reciprocal_rank_tail = np.sum(1.0/np.asarray(filter_rank_tail, dtype=np.float32))/n_test\n",
    "\n",
    "    hit1_head, hit1_tail, hit1 = hit(rank_head, rank_tail, 1)\n",
    "    filter_hit1_head, filter_hit1_tail, filter_hit1 = hit(filter_rank_head, filter_rank_tail, 1)\n",
    "    hit3_head, hit3_tail, hit3 = hit(rank_head, rank_tail, 3)\n",
    "    filter_hit3_head, filter_hit3_tail, filter_hit3 = hit(filter_rank_head, filter_rank_tail, 3)\n",
    "    hit10_head, hit10_tail, hit10 = hit(rank_head, rank_tail, 10)\n",
    "    filter_hit10_head, filter_hit10_tail, filter_hit10 = hit(filter_rank_head, filter_rank_tail, 10)\n",
    "\n",
    "\n",
    "    print('iter:%d --MR: %.2f  --MRR: %.2f  --hit@1: %.2f   --hit@3: %.2f    --hit@10: %.2f' %(n_iter, (mean_rank_head+ mean_rank_tail)/2, \n",
    "                                                            (mean_reciprocal_rank_head + mean_reciprocal_rank_tail)/2, \n",
    "                                                            hit1, hit3, hit3))\n",
    "    print('iter:%d --FMR: %.2f --FMRR: %.2f --Fhit@1: %.2f  --Fhit@3: %.2f   --Fhit@10: %.2f' %(n_iter, (filter_mean_rank_head+ filter_mean_rank_tail)/2, \n",
    "                                                                    (filter_mean_reciprocal_rank_head + filter_mean_reciprocal_rank_tail)/2,\n",
    "                                                                    filter_hit1, filter_hit3, filter_hit10)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x12e4616d8>\n",
      "loading entity2id.txt ...\n",
      "loading reltion2id.txt ...\n",
      "entity number: 14541\n",
      "relation number: 237\n",
      "training triple number: 272115\n",
      "testing triple number: 20466\n",
      "valid triple number: 17535\n",
      "finish preparing data. \n",
      "WARNING:tensorflow:From /Users/zhangwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "finishing initializing\n",
      "WARNING:tensorflow:From /Users/zhangwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-2-88798284c406>:245: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "# 设置参数等\n",
    "args  = Args()\n",
    "args.data_dir = './data/FB15k-237/'\n",
    "args.learning_rate = 0.005\n",
    "args.batch_size = 2048\n",
    "args.max_iter = 200\n",
    "args.optimizer = 'adam'\n",
    "args.dimension = 300\n",
    "args.margin = 3\n",
    "args.norm = 'L2'\n",
    "args.evaluation_size = 500\n",
    "args.save_dir = 'output/'\n",
    "args.negative_sampling = 'bern'\n",
    "args.evaluate_per_iteration = 1\n",
    "args.evaluate_worker = 3\n",
    "args.regularizer_weight = 1e-7\n",
    "args.n_test = 100\n",
    "args.save_per = 100\n",
    "args.n_worker = 5\n",
    "args.max_iter = 20\n",
    "\n",
    "print(args)\n",
    "model = DistMult(negative_sampling=args.negative_sampling, data_dir=args.data_dir,\n",
    "                learning_rate=args.learning_rate, batch_size=args.batch_size,\n",
    "                max_iter=args.max_iter, margin=args.margin, \n",
    "                dimension=args.dimension, norm=args.norm, evaluation_size=args.evaluation_size, \n",
    "                regularizer_weight = args.regularizer_weight)\n",
    "\n",
    "train_triple_positive_input, train_triple_negative_input, loss, op_train, norm_entity = train_operation(model, learning_rate = args.learning_rate, margin = args.margin, optimizer_str = args.optimizer)\n",
    "test_triple, head_rank, tail_rank , norm_head_rank, norm_tail_rank= test_operation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zhangwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "iter[0] ---loss: 111.96902 ---time: 179.15 ---prepare time : 0.50\n",
      "Model saved at ./save/DistMult_0.ckpt\n",
      "iter:0 --MR: 6973.84  --MRR: 0.00  --hit@1: 0.00   --hit@3: 0.00    --hit@10: 0.00\n",
      "iter:0 --FMR: 6791.24 --FMRR: 0.00 --Fhit@1: 0.00  --Fhit@3: 0.00   --Fhit@10: 0.00\n",
      "iter[1] ---loss: 100.37770 ---time: 141.22 ---prepare time : 0.36\n",
      "iter:1 --MR: 3672.12  --MRR: 0.08  --hit@1: 0.06   --hit@3: 0.08    --hit@10: 0.08\n",
      "iter:1 --FMR: 3508.59 --FMRR: 0.12 --Fhit@1: 0.10  --Fhit@3: 0.12   --Fhit@10: 0.17\n",
      "iter[2] ---loss: 86.79653 ---time: 156.98 ---prepare time : 0.41\n",
      "iter:2 --MR: 2673.16  --MRR: 0.05  --hit@1: 0.02   --hit@3: 0.07    --hit@10: 0.07\n",
      "iter:2 --FMR: 2472.02 --FMRR: 0.11 --Fhit@1: 0.07  --Fhit@3: 0.14   --Fhit@10: 0.19\n",
      "iter[3] ---loss: 57.74097 ---time: 149.05 ---prepare time : 0.35\n",
      "iter:3 --MR: 1123.46  --MRR: 0.05  --hit@1: 0.02   --hit@3: 0.04    --hit@10: 0.04\n",
      "iter:3 --FMR: 975.15 --FMRR: 0.10 --Fhit@1: 0.05  --Fhit@3: 0.10   --Fhit@10: 0.19\n",
      "iter[4] ---loss: 36.23117 ---time: 131.45 ---prepare time : 0.33\n",
      "iter:4 --MR: 865.30  --MRR: 0.09  --hit@1: 0.05   --hit@3: 0.10    --hit@10: 0.10\n",
      "iter:4 --FMR: 715.25 --FMRR: 0.16 --Fhit@1: 0.09  --Fhit@3: 0.18   --Fhit@10: 0.28\n",
      "iter[5] ---loss: 29.85469 ---time: 131.38 ---prepare time : 0.33\n",
      "iter:5 --MR: 1315.52  --MRR: 0.10  --hit@1: 0.06   --hit@3: 0.09    --hit@10: 0.09\n",
      "iter:5 --FMR: 1053.90 --FMRR: 0.17 --Fhit@1: 0.12  --Fhit@3: 0.16   --Fhit@10: 0.24\n",
      "iter[6] ---loss: 27.23370 ---time: 130.46 ---prepare time : 0.33\n",
      "iter:6 --MR: 797.73  --MRR: 0.08  --hit@1: 0.03   --hit@3: 0.07    --hit@10: 0.07\n",
      "iter:6 --FMR: 600.81 --FMRR: 0.20 --Fhit@1: 0.12  --Fhit@3: 0.24   --Fhit@10: 0.35\n",
      "iter[7] ---loss: 25.46609 ---time: 146.49 ---prepare time : 0.38\n",
      "iter:7 --MR: 1219.23  --MRR: 0.11  --hit@1: 0.07   --hit@3: 0.11    --hit@10: 0.11\n",
      "iter:7 --FMR: 1012.73 --FMRR: 0.19 --Fhit@1: 0.13  --Fhit@3: 0.18   --Fhit@10: 0.33\n",
      "iter[8] ---loss: 24.17620 ---time: 153.80 ---prepare time : 0.39\n",
      "iter:8 --MR: 748.05  --MRR: 0.12  --hit@1: 0.07   --hit@3: 0.13    --hit@10: 0.13\n",
      "iter:8 --FMR: 598.50 --FMRR: 0.23 --Fhit@1: 0.15  --Fhit@3: 0.24   --Fhit@10: 0.38\n",
      "iter[9] ---loss: 23.08618 ---time: 154.25 ---prepare time : 0.37\n",
      "iter:9 --MR: 594.65  --MRR: 0.12  --hit@1: 0.06   --hit@3: 0.12    --hit@10: 0.12\n",
      "iter:9 --FMR: 378.65 --FMRR: 0.21 --Fhit@1: 0.14  --Fhit@3: 0.22   --Fhit@10: 0.37\n",
      "iter[10] ---loss: 22.20838 ---time: 134.06 ---prepare time : 0.34\n",
      "iter:10 --MR: 1234.09  --MRR: 0.08  --hit@1: 0.03   --hit@3: 0.07    --hit@10: 0.07\n",
      "iter:10 --FMR: 1004.27 --FMRR: 0.16 --Fhit@1: 0.10  --Fhit@3: 0.17   --Fhit@10: 0.29\n",
      "iter[11] ---loss: 21.40843 ---time: 145.39 ---prepare time : 0.36\n",
      "iter:11 --MR: 865.20  --MRR: 0.10  --hit@1: 0.06   --hit@3: 0.10    --hit@10: 0.10\n",
      "iter:11 --FMR: 672.80 --FMRR: 0.18 --Fhit@1: 0.12  --Fhit@3: 0.21   --Fhit@10: 0.31\n",
      "iter[12] ---loss: 20.70802 ---time: 138.76 ---prepare time : 0.34\n",
      "iter:12 --MR: 622.17  --MRR: 0.10  --hit@1: 0.04   --hit@3: 0.10    --hit@10: 0.10\n",
      "iter:12 --FMR: 457.10 --FMRR: 0.18 --Fhit@1: 0.11  --Fhit@3: 0.21   --Fhit@10: 0.35\n",
      "iter[13] ---loss: 20.05850 ---time: 204.98 ---prepare time : 0.53\n",
      "iter:13 --MR: 795.53  --MRR: 0.13  --hit@1: 0.07   --hit@3: 0.14    --hit@10: 0.14\n",
      "iter:13 --FMR: 554.81 --FMRR: 0.22 --Fhit@1: 0.15  --Fhit@3: 0.24   --Fhit@10: 0.35\n",
      "iter[14] ---loss: 19.46632 ---time: 172.63 ---prepare time : 0.43\n",
      "iter:14 --MR: 741.01  --MRR: 0.13  --hit@1: 0.07   --hit@3: 0.12    --hit@10: 0.12\n",
      "iter:14 --FMR: 527.94 --FMRR: 0.21 --Fhit@1: 0.11  --Fhit@3: 0.25   --Fhit@10: 0.38\n",
      "iter[15] ---loss: 19.07239 ---time: 132.43 ---prepare time : 0.33\n",
      "iter:15 --MR: 847.09  --MRR: 0.12  --hit@1: 0.05   --hit@3: 0.14    --hit@10: 0.14\n",
      "iter:15 --FMR: 623.00 --FMRR: 0.23 --Fhit@1: 0.14  --Fhit@3: 0.27   --Fhit@10: 0.41\n",
      "iter[16] ---loss: 18.67151 ---time: 129.34 ---prepare time : 0.32\n",
      "iter:16 --MR: 834.88  --MRR: 0.10  --hit@1: 0.05   --hit@3: 0.10    --hit@10: 0.10\n",
      "iter:16 --FMR: 634.10 --FMRR: 0.20 --Fhit@1: 0.12  --Fhit@3: 0.23   --Fhit@10: 0.36\n",
      "iter[17] ---loss: 18.33356 ---time: 128.92 ---prepare time : 0.31\n",
      "iter:17 --MR: 434.40  --MRR: 0.10  --hit@1: 0.04   --hit@3: 0.10    --hit@10: 0.10\n",
      "iter:17 --FMR: 324.92 --FMRR: 0.20 --Fhit@1: 0.11  --Fhit@3: 0.23   --Fhit@10: 0.41\n",
      "iter[18] ---loss: 17.88594 ---time: 129.93 ---prepare time : 0.31\n",
      "iter:18 --MR: 897.46  --MRR: 0.11  --hit@1: 0.06   --hit@3: 0.10    --hit@10: 0.10\n",
      "iter:18 --FMR: 678.23 --FMRR: 0.18 --Fhit@1: 0.10  --Fhit@3: 0.20   --Fhit@10: 0.34\n",
      "iter[19] ---loss: 17.56188 ---time: 129.66 ---prepare time : 0.31\n",
      "Model saved at ./save/DistMult_19.ckpt\n",
      "iter:19 --MR: 659.69  --MRR: 0.12  --hit@1: 0.06   --hit@3: 0.12    --hit@10: 0.12\n",
      "iter:19 --FMR: 434.83 --FMRR: 0.21 --Fhit@1: 0.13  --Fhit@3: 0.24   --Fhit@10: 0.39\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = False\n",
    "config.log_device_placement = False\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction=0.68\n",
    "session = tf.Session(config=config)\n",
    "session.as_default()\n",
    "\n",
    "tf.initialize_all_variables().run(session=session)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "for n_iter in range(args.max_iter):\n",
    "    accu_loss =0.\n",
    "    batch = 0\n",
    "    num_batch = model.num_triple_train/args.batch_size\n",
    "    start_time = timeit.default_timer()\n",
    "    prepare_time = 0.\n",
    "\n",
    "    for tp, tn , t in  model.training_data_batch(batch_size= args.batch_size):\n",
    "        l, _, norm_e = session.run([loss, op_train, norm_entity], {train_triple_positive_input:tp, train_triple_negative_input: tn})\n",
    "        accu_loss += l\n",
    "        batch += 1\n",
    "        print('[%.2f sec](%d/%d): -- loss: %.5f' %(timeit.default_timer()-start_time, batch, num_batch , l), end='\\r')\n",
    "        prepare_time += t\n",
    "    if n_iter%1 == 0:\n",
    "        print('iter[%d] ---loss: %.5f ---time: %.2f ---prepare time : %.2f' %(n_iter, accu_loss, timeit.default_timer()-start_time, prepare_time))\n",
    "\n",
    "    if n_iter % args.save_per == 0 or n_iter ==0 or n_iter == args.max_iter-1:\n",
    "        save_path = saver.save(session, os.path.join('./save/DistMult_' + str(n_iter) + '.ckpt'))\n",
    "        print('Model saved at %s' % save_path)\n",
    "\n",
    "    if n_iter %args.evaluate_per_iteration == 0 or n_iter ==0 or n_iter == args.max_iter-1:\n",
    "        rank_head = []\n",
    "        rank_tail = []\n",
    "        filter_rank_head = []\n",
    "        filter_rank_tail = []\n",
    "\n",
    "        norm_rank_head = []\n",
    "        norm_rank_tail = []\n",
    "        norm_filter_rank_head = []\n",
    "        norm_filter_rank_tail = []\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        testing_data = model.testing_data\n",
    "        hr_t = model.hr_t\n",
    "        tr_h = model.tr_h\n",
    "        n_test = args.n_test\n",
    "        test(n_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模型\n",
    "predicted_tail = []\n",
    "norm_predicted_tail = []\n",
    "predicted_head = []\n",
    "norm_predicted_head = []\n",
    "\n",
    "rank_head = []\n",
    "rank_tail = []\n",
    "filter_rank_head = []\n",
    "filter_rank_tail = []\n",
    "\n",
    "norm_rank_head = []\n",
    "norm_rank_tail = []\n",
    "norm_filter_rank_head = []\n",
    "norm_filter_rank_tail = []\n",
    "\n",
    "start = timeit.default_timer()\n",
    "testing_data = model.testing_data\n",
    "# hr_t = model.hr_t\n",
    "# tr_h = model.tr_h\n",
    "n_test = args.n_test\n",
    "if n_iter == args.max_iter-1:\tn_test = model.num_triple_test\n",
    "predicted_tail = []\n",
    "norm_predicted_tail = []\n",
    "predicted_head = []\n",
    "norm_predicted_head = []\n",
    "test(n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试一个样本,、\n",
    "# 下面我们选取单个样本进行测试，\n",
    "# 输出tail以及head prediction的rank，并出给top k的list，\n",
    "# 这里k取值长度为rank+3，方便大家观察排序为正确实体前后的实体分别是什么。\n",
    "\n",
    "sample = ['北京市','包含区','海淀区']\n",
    "# sample = ['鼓楼区','位于省','江苏省']\n",
    "# sample = ['合肥市','位于省','安徽省']\n",
    "# sample = ['浙江大学','位于市','杭州市']\n",
    "trp = [model.entity2id[sample[0]],\n",
    "       model.relation2id[sample[1]],\n",
    "       model.entity2id[sample[2]]]\n",
    "print(sample, trp)\n",
    "\n",
    "hrank, fhrank, trank, ftrank, norm_hrank, norm_fhrank, norm_trank, norm_ftrank, \\\n",
    "            predicted_head_tmp, predicted_tail_tmp, norm_predicted_head_tmp, norm_predicted_tail_tmp = test_one_sample(model, trp, session)\n",
    "\n",
    "tail_prediction = [model.id2entity[ent] for ent in predicted_tail_tmp]\n",
    "head_prediction = [model.id2entity[ent] for ent in predicted_head_tmp]\n",
    "\n",
    "norm_tail_prediction = [model.id2entity[ent] for ent in norm_predicted_tail_tmp]\n",
    "norm_head_prediction = [model.id2entity[ent] for ent in norm_predicted_head_tmp]\n",
    "\n",
    "print('-- trank:', trank, tail_prediction[:trank +3])\n",
    "print('-- norm_trank', norm_trank, norm_tail_prediction[:norm_trank +3])\n",
    "print('-- hrank', hrank, head_prediction[:hrank +3])\n",
    "print('-- norm_hrank', norm_hrank, norm_head_prediction[:norm_hrank+3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们按照设置的参数跑了20个iteration，可以看到ComplEx在FB15k-237的数据集上已经有了明显的预测效果。\n",
    "本demo中不包括调参的部分，有兴趣的同学可以阅读原文并自行尝试不同的参数组合，并观察对模型训练和预测结果的影响 :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
